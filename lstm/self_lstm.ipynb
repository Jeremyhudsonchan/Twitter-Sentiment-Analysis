{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AfxzlOJ2NaI",
        "outputId": "a4554605-24f5-4ee8-9eee-867cb89d380b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 38.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 58.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJtRGs7axS7Q"
      },
      "outputs": [],
      "source": [
        "import sys, os, time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc4w3iHXx0Na",
        "outputId": "dfe2cbe7-7b2c-4229-ebab-3d41a1e578b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGo_HrGqx8Zn",
        "outputId": "47485d70-d04d-4eb0-e1ec-769966fdb437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lstm  saved_tensors  training.1600000.processed.noemoticon.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/nns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpAXPyvLybTL"
      },
      "outputs": [],
      "source": [
        "load_dir = '/content/drive/MyDrive/nns/saved_tensors/'\n",
        "train_seq = torch.load(load_dir+'train_seq.pt')\n",
        "train_y = torch.load(load_dir+'train_y.pt')\n",
        "\n",
        "val_seq = torch.load(load_dir+'val_seq.pt')\n",
        "val_y = torch.load(load_dir+'val_y.pt')\n",
        "\n",
        "test_seq = torch.load(load_dir+'test_seq.pt')\n",
        "test_y = torch.load(load_dir+'test_y.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x-7m1KU1kva"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/nns/training.1600000.processed.noemoticon.csv\", encoding = \"latin-1\", low_memory=False)\n",
        "df = df[['text', 'target']]\n",
        "df.columns = ['text', 'target']\n",
        "df['target'] = df['target'].replace(4, 1)\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['target'], random_state=2022, test_size=0.3, stratify=df['target'])\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, random_state=2022, test_size=0.5, stratify=temp_labels)\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "max_seq_len = max(seq_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLDaOM4-16nG",
        "outputId": "d4a523f2-2e24-4a62-ad91-58cd497f7a21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64TuuEAC2uO1"
      },
      "outputs": [],
      "source": [
        "# Convert Integer Sequences to Tensors\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s1T0V5a4EIT"
      },
      "outputs": [],
      "source": [
        "train_seq = tf.cast(torch.tensor(tokens_train['input_ids']), tf.float32)\n",
        "train_mask = tf.cast(torch.tensor(tokens_train['attention_mask']), tf.float32)\n",
        "train_y = tf.cast(torch.tensor(train_labels.tolist()), tf.float32)\n",
        "\n",
        "val_seq = tf.cast(torch.tensor(tokens_val['input_ids']), tf.float32)\n",
        "val_mask = tf.cast(torch.tensor(tokens_val['attention_mask']), tf.float32)\n",
        "val_y = tf.cast(torch.tensor(val_labels.tolist()), tf.float32)\n",
        "\n",
        "test_seq = tf.cast(torch.tensor(tokens_test['input_ids']), tf.float32)\n",
        "test_mask = tf.cast(torch.tensor(tokens_test['attention_mask']), tf.float32)\n",
        "test_y = tf.cast(torch.tensor(test_labels.tolist()), tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWlP9niOynzK"
      },
      "outputs": [],
      "source": [
        "#NN Hyper-Parameters\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "learning_rate = 1e-3\n",
        "max_seq_len = 35\n",
        "embedding_dim = 512\n",
        "lstm_units = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9viodhqoytPW"
      },
      "outputs": [],
      "source": [
        "#Model create lstm\n",
        "class LSTMClassifier(tf.keras.Model):\n",
        "    def __init__(self, lstm_units, max_seq_len, embedding_dim, batch_size):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.lstm_units = lstm_units\n",
        "        self.batch_size = batch_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.embedding = tf.keras.layers.Embedding(max_seq_len, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.lstm_units, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        \n",
        "    def call(self, inputs, hidden):\n",
        "        x = self.embedding(inputs)\n",
        "        output, state_h, state_c = self.lstm(x, initial_state=hidden)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        output = self.dense(output)\n",
        "        return output, state_h, state_c\n",
        "    \n",
        "    def init_hidden_state(self):\n",
        "        return (tf.zeros((self.batch_size, self.lstm_units)), tf.zeros((self.batch_size, self.lstm_units)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train model\n",
        "model = LSTMClassifier(lstm_units, max_seq_len, embedding_dim, batch_size)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define function to train the model\n",
        "@tf.function\n",
        "def train_step(model, inputs, labels, hidden):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _, _ = model(inputs, hidden)\n",
        "        loss = loss_object(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(labels, predictions)\n",
        "\n",
        "# define function to evaluate the model\n",
        "@tf.function\n",
        "def val_step(model, inputs, labels, hidden):\n",
        "    predictions, _, _ = model(inputs, hidden)\n",
        "    v_loss = loss_object(labels, predictions)\n",
        "    val_loss(v_loss)\n",
        "    val_accuracy(labels, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXL2ig210RRU"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "for epoch in range(num_epochs):\n",
        "    hidden = model.init_hidden_state()\n",
        "    for (batch, (inputs, labels)) in enumerate(train_seq.take(batch_size)):\n",
        "        train_step(model, inputs, labels, hidden)\n",
        "        template = 'Epoch {}, Batch {}, Loss: {}, Accuracy: {}'\n",
        "        print(template.format(epoch+1,\n",
        "                              batch,\n",
        "                              train_loss.result(),\n",
        "                              train_accuracy.result()*100))\n",
        "    hidden = model.init_hidden_state()\n",
        "    for (batch, (inputs, labels)) in enumerate(val_seq):\n",
        "        val_step(model, inputs, labels, hidden)\n",
        "        template = 'Epoch {}, Batch {}, Loss: {}, Accuracy: {}'\n",
        "        print(template.format(epoch+1,\n",
        "                              batch,\n",
        "                              val_loss.result(),\n",
        "                              val_accuracy.result()*100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
