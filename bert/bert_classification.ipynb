{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.9/site-packages (4.24.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.9/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/Cellar/sip/6.5.0/libexec/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from transformers) (1.23.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/Cellar/sip/6.5.0/libexec/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# finetuning bert language model for classification\n",
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in pretrained bert model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in dataset\n",
    "df = pd.read_csv(\"/Users/jeremyhudsonchan/Dropbox/Files/Github_Repos/Twitter-Sentiment-Analysis/data/sampled/training.1600000.processed.noemoticon.csv\", encoding = \"latin-1\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune bert model to dataset to classify tweets as positive or negative\n",
    "# create dataset class\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.tweet = dataframe.text\n",
    "        self.targets = self.data.target\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        tweet = str(self.tweet[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            tweet,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'tweet_text': tweet,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader class\n",
    "class TweetDataLoader:\n",
    "    def __init__(self, tweets, tokenizer, max_len, batch_size):\n",
    "        self.tweets = tweets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.data_loader = DataLoader(\n",
    "            self.tweets,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "    def return_data_loader(self):\n",
    "        return self.data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model class\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training class\n",
    "class TweetTrainer:\n",
    "    def __init__(self, model, data_loader, optimizer, device, scheduler=None):\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "        self.loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    def train(self):\n",
    "        self.model = self.model.train()\n",
    "        losses = []\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for d in self.data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(self.device)\n",
    "            attention_mask = d[\"attention_mask\"].to(self.device)\n",
    "            targets = d[\"targets\"].to(self.device)\n",
    "\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = self.loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        return correct_predictions.double() / len(self.data_loader.dataset), np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create testing class\n",
    "class TweetTester:\n",
    "    def __init__(self, model, data_loader, device):\n",
    "        self.model = model\n",
    "        self.data_loader = data_loader\n",
    "        self.device = device\n",
    "        self.loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    def test(self):\n",
    "        self.model = self.model.eval()\n",
    "        losses = []\n",
    "        correct_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for d in self.data_loader:\n",
    "                input_ids = d[\"input_ids\"].to(self.device)\n",
    "                attention_mask = d[\"attention_mask\"].to(self.device)\n",
    "                targets = d[\"targets\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "\n",
    "                correct_predictions += torch.sum(preds == targets)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        return correct_predictions.double() / len(self.data_loader.dataset), np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training function\n",
    "def train(model, data_loader, optimizer, device, scheduler=None):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create testing function\n",
    "def test(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create main function\n",
    "def main():\n",
    "    # create dataset\n",
    "    train_dataset = TweetDataset(\n",
    "        tweets=train_df.text.to_numpy(),\n",
    "        targets=train_df.target.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    test_dataset = TweetDataset(\n",
    "        tweets=test_df.text.to_numpy(),\n",
    "        targets=test_df.target.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    # create dataloader\n",
    "    train_data_loader = TweetDataLoader(\n",
    "        tweets=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN,\n",
    "        batch_size=BATCH_SIZE\n",
    "    ).return_data_loader()\n",
    "\n",
    "    test_data_loader = TweetDataLoader(\n",
    "        tweets=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN,\n",
    "        batch_size=BATCH_SIZE\n",
    "    ).return_data_loader()\n",
    "\n",
    "    # create model\n",
    "    model = SentimentClassifier(len(class_names))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # create optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n",
    "    total_steps = len(train_data_loader) * EPOCHS\n",
    "    \n",
    "    # create scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # create trainer and tester\n",
    "    trainer = TweetTrainer(\n",
    "        model=model,\n",
    "        data_loader=train_data_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "    \n",
    "    tester = TweetTester(\n",
    "        model=model,\n",
    "        data_loader=test_data_loader,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # create history\n",
    "    history = defaultdict(list)\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    # start training\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "        print(\"-\" * 10)\n",
    "        \n",
    "        train_acc, train_loss = trainer.train()\n",
    "        print(f\"Train loss {train_loss} accuracy {train_acc}\")\n",
    "        \n",
    "        test_acc, test_loss = tester.test()\n",
    "        print(f\"Test loss {test_loss} accuracy {test_acc}\")\n",
    "        print()\n",
    "        \n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"test_acc\"].append(test_acc)\n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        \n",
    "        if test_acc > best_accuracy:\n",
    "            torch.save(model.state_dict(), \"best_model_state.bin\")\n",
    "            best_accuracy = test_acc\n",
    "    \n",
    "    print(f\"Best Test Accuracy: {best_accuracy}\")\n",
    "    \n",
    "    # plot history\n",
    "    plt.plot(history[\"train_acc\"], label=\"train accuracy\")\n",
    "    plt.plot(history[\"test_acc\"], label=\"test accuracy\")\n",
    "    plt.title(\"Training history\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the parameters, then run model\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LEN = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
