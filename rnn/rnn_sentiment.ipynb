{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK4hGtwYDM9x",
        "outputId": "ece6ebc5-e5c2-4dab-ddbb-958f09d44b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.8/dist-packages (3.8.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (5.2.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.28.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (22.11.23)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.50.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.14.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim==3.8.3\n",
        "!pip install keras --upgrade\n",
        "!pip install pandas --upgrade\n",
        "!pip install tensorflow --upgrade\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Matplot\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Keras\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "from keras import utils\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Word2vec\n",
        "import gensim\n",
        "\n",
        "#transformers\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# Utility\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter\n",
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "import itertools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NeMm2b1DOoQ",
        "outputId": "8e4f6931-37d7-4344-f5f3-69ce8f52edbd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vocab_size = 290419\n",
        "# DATASET\n",
        "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "# TEXT CLENAING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "# WORD2VEC \n",
        "W2V_SIZE = 300\n",
        "W2V_WINDOW = 7\n",
        "W2V_EPOCH = 32\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)"
      ],
      "metadata": {
        "id": "c0zpGRIODS0I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoECcrwyDUzt",
        "outputId": "2a380e7a-eead-42b7-8ceb-e424d0bf4a39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD DATASET\n",
        "main_dir = '/content/drive/MyDrive/nns/'\n",
        "df = pd.read_csv(main_dir+'training.1600000.processed.noemoticon.csv', encoding = DATASET_ENCODING,  names=DATASET_COLUMNS)\n",
        "neg_df = df[df.target == 0].sample(n=25000, random_state=5)\n",
        "pos_df = df[df.target == 4].sample(n=25000, random_state=5)"
      ],
      "metadata": {
        "id": "lPYzG7eRJZur"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode_map = {0: \"NEGATIVE\", 2:\"NEUTRAL\", 4: \"POSITIVE\"}\n",
        "def decode_sentiment(label):\n",
        "    return decode_map[int(label)]"
      ],
      "metadata": {
        "id": "tht_a-bQJrqD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "def preprocess(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = str(text).lower().strip()\n",
        "    text = re.sub(\"\\n\", \"\", text)\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "fqDFUeI-qbKE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df.text = df.text.apply(lambda x: preprocess(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4d7cY9IKFiN",
        "outputId": "ce18775b-77b9-4e67-a72c-bf425cfcebf8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 53 s, sys: 273 ms, total: 53.3 s\n",
            "Wall time: 55.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\n",
        "print(\"TRAIN size:\", len(df_train))\n",
        "print(\"TEST size:\", len(df_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEmyHUISKJap",
        "outputId": "1e760d43-7a20-46d5-da96-c2a63b63984b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN size: 1280000\n",
            "TEST size: 320000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD POLITICIANS TWEETS\n",
        "twt_load_dir = main_dir+'twitter_api_data/original/'\n",
        "\n",
        "# tweets regarding the politicans\n",
        "mehmet_oz_df = pd.read_csv(twt_load_dir+\"adam_laxalt.csv\", encoding =DATASET_ENCODING)\n",
        "john_fetterman_df = pd.read_csv(twt_load_dir+\"john_fetterman.csv\")\n",
        "adam_laxalt_df = pd.read_csv(twt_load_dir+\"adam_laxalt.csv\")\n",
        "catherine_cortez_masto_df = pd.read_csv(twt_load_dir+\"catherine_cortez_masto.csv\")\n",
        "ron_johnson_df = pd.read_csv(twt_load_dir+\"ron_johnson.csv\")\n",
        "mandela_barnes_df = pd.read_csv(twt_load_dir+\"mandela_barnes.csv\")\n",
        "donald_bolduc_df = pd.read_csv(twt_load_dir+\"donald_bolduc.csv\")\n",
        "maggie_hassan_df = pd.read_csv(twt_load_dir+\"maggie_hassan.csv\")\n",
        "ted_budd_df = pd.read_csv(twt_load_dir+\"ted_budd.csv\")\n",
        "cheri_beasly_df = pd.read_csv(twt_load_dir+\"cheri_beasly.csv\")\n",
        "joe_pinion_df = pd.read_csv(twt_load_dir+\"joe_pinion.csv\")\n",
        "charles_schumer_df = pd.read_csv(twt_load_dir+\"charles_schumer.csv\")\n",
        "jd_vance_df = pd.read_csv(twt_load_dir+\"jd_vance.csv\")\n",
        "tim_ryan_df = pd.read_csv(twt_load_dir+\"tim_ryan.csv\")"
      ],
      "metadata": {
        "id": "RZEpR2YuEXtv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD RNN MODEL\n",
        "main_dir = '/content/drive/MyDrive/nns/'\n",
        "nn_load_dir = main_dir+'saved_nn_models/'\n",
        "model = tf.keras.models.load_model(nn_load_dir+'RNN.h5')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq09IbHXGj2h",
        "outputId": "69d61488-5cc8-481a-f742-fe6aae2a2335"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 300, 300)          87125700  \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 300, 300)          0         \n",
            "                                                                 \n",
            " gru_7 (GRU)                 (None, 1024)              4073472   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 1025      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 91,200,197\n",
            "Trainable params: 4,074,497\n",
            "Non-trainable params: 87,125,700\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sentiment(score, include_neutral=False):\n",
        "    if include_neutral:        \n",
        "        label = NEUTRAL\n",
        "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "\n",
        "        return label\n",
        "    else:\n",
        "        return 0 if score < 0.5 else 1"
      ],
      "metadata": {
        "id": "PseWT6o3KZW7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD TOKENIZER\n",
        "tokenizer_path = main_dir+'tokenizer.pkl'\n",
        "\n",
        "with open(tokenizer_path, 'rb') as f:\n",
        "  tokenizer = pickle.load(f)"
      ],
      "metadata": {
        "id": "r3bGr9lgXuVd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, include_neutral=False):\n",
        "    # Tokenize text\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
        "    # Predict\n",
        "    score = model.predict([x_test], verbose=0)[0]\n",
        "    # Decode sentiment\n",
        "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "    return label"
      ],
      "metadata": {
        "id": "nj4ZHh0E0rrQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(df):\n",
        "  df.Tweet = df.Tweet.apply(lambda x: preprocess(x))\n",
        "  df['sentiment'] = 0\n",
        "  length = len(df)\n",
        "  for i in range(length):\n",
        "    text = df.Tweet[i]\n",
        "    label = predict(text, include_neutral=False)\n",
        "    df.at[i, 'sentiment'] = label\n",
        "    "
      ],
      "metadata": {
        "id": "lV3quS_72fON"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run model on all csvs\n",
        "run_model(mehmet_oz_df)\n",
        "run_model(john_fetterman_df)\n",
        "run_model(adam_laxalt_df)\n",
        "run_model(catherine_cortez_masto_df)\n",
        "run_model(ron_johnson_df)\n",
        "run_model(mandela_barnes_df)\n",
        "run_model(donald_bolduc_df)\n",
        "run_model(maggie_hassan_df)\n",
        "run_model(ted_budd_df)\n",
        "run_model(cheri_beasly_df)\n",
        "run_model(joe_pinion_df)\n",
        "run_model(charles_schumer_df)\n",
        "run_model(jd_vance_df)\n",
        "run_model(tim_ryan_df)"
      ],
      "metadata": {
        "id": "N8YPBSYo3rSZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get number of positive and negatives tweets from each df\n",
        "def get_sentiment(df):\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if row['sentiment'] == 0:\n",
        "            neg += 1\n",
        "        else:\n",
        "            pos += 1\n",
        "    return pos, neg\n",
        "\n",
        "# get pos/neg ,pos/all, neg/all ratio for all dfs\n",
        "def get_ratios(df):\n",
        "    pos, neg = get_sentiment(df)\n",
        "    pos_all = pos / (pos + neg)\n",
        "    neg_all = neg / (pos + neg)\n",
        "    pos_neg = pos / neg\n",
        "    # round all ratios to 2 decimal places\n",
        "    pos_all = round(pos_all, 2)\n",
        "    neg_all = round(neg_all, 2)\n",
        "    pos_neg = round(pos_neg, 2)\n",
        "    return pos_all, neg_all, pos_neg\n",
        "\n",
        "# sum number of positive and negatives tweets from list of df\n",
        "def sum_sentiment(dfs):\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "    for df in dfs:\n",
        "        pos_df, neg_df = get_sentiment(df)\n",
        "        pos += pos_df\n",
        "        neg += neg_df\n",
        "    return pos, neg\n",
        "\n",
        "# average the ratios of winners and losers\n",
        "def avg_ratios(dfs):\n",
        "    pos_all = 0\n",
        "    neg_all = 0\n",
        "    pos_neg = 0\n",
        "    for df in dfs:\n",
        "        pos_all_df, neg_all_df, pos_neg_df = get_ratios(df)\n",
        "        pos_all += pos_all_df\n",
        "        neg_all += neg_all_df\n",
        "        pos_neg += pos_neg_df\n",
        "    pos_all = pos_all / len(dfs)\n",
        "    neg_all = neg_all / len(dfs)\n",
        "    pos_neg = pos_neg / len(dfs)\n",
        "    return pos_all, neg_all, pos_neg"
      ],
      "metadata": {
        "id": "MYsQK8Y06fIE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get number of positive and negatives tweets from each df\n",
        "pos, neg = get_sentiment(mehmet_oz_df)\n",
        "print(\"Mehmet Oz: \", pos, neg)\n",
        "pos, neg = get_sentiment(john_fetterman_df)\n",
        "print(\"John Fetterman: \", pos, neg)\n",
        "pos, neg = get_sentiment(adam_laxalt_df)\n",
        "print(\"Adam Laxalt: \", pos, neg)\n",
        "pos, neg = get_sentiment(catherine_cortez_masto_df)\n",
        "print(\"Catherine Cortez Masto: \", pos, neg)\n",
        "pos, neg = get_sentiment(ron_johnson_df)\n",
        "print(\"Ron Johnson: \", pos, neg)\n",
        "pos, neg = get_sentiment(mandela_barnes_df)\n",
        "print(\"Mandela Barnes: \", pos, neg)\n",
        "pos, neg = get_sentiment(donald_bolduc_df)\n",
        "print(\"Donald Bolduc: \", pos, neg)\n",
        "pos, neg = get_sentiment(maggie_hassan_df)\n",
        "print(\"Maggie Hassan: \", pos, neg)\n",
        "pos, neg = get_sentiment(ted_budd_df)\n",
        "print(\"Ted Budd: \", pos, neg)\n",
        "pos, neg = get_sentiment(cheri_beasly_df)\n",
        "print(\"Cheri Beasley: \", pos, neg)\n",
        "pos, neg = get_sentiment(joe_pinion_df)\n",
        "print(\"Joe Pinion: \", pos, neg)\n",
        "pos, neg = get_sentiment(charles_schumer_df)\n",
        "print(\"Charles Schumer: \", pos, neg)\n",
        "pos, neg = get_sentiment(jd_vance_df)\n",
        "print(\"JD Vance: \", pos, neg)\n",
        "pos, neg = get_sentiment(tim_ryan_df)\n",
        "print(\"Tim Ryan: \", pos, neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zIQoy5L6mAb",
        "outputId": "93366d16-99c6-4aa5-e963-c32f102b6379"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mehmet Oz:  145 75\n",
            "John Fetterman:  75 425\n",
            "Adam Laxalt:  145 75\n",
            "Catherine Cortez Masto:  100 30\n",
            "Ron Johnson:  485 15\n",
            "Mandela Barnes:  340 100\n",
            "Donald Bolduc:  15 10\n",
            "Maggie Hassan:  115 15\n",
            "Ted Budd:  60 440\n",
            "Cheri Beasley:  155 10\n",
            "Joe Pinion:  30 20\n",
            "Charles Schumer:  115 60\n",
            "JD Vance:  305 195\n",
            "Tim Ryan:  300 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get ratios of pos:neg tweets from all dfs\n",
        "pos_all, neg_all, pos_neg = get_ratios(mehmet_oz_df)\n",
        "print(\"Mehmet Oz: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(john_fetterman_df)\n",
        "print(\"John Fetterman: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(adam_laxalt_df)\n",
        "print(\"Adam Laxalt: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(catherine_cortez_masto_df)\n",
        "print(\"Catherine Cortez Masto: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(ron_johnson_df)\n",
        "print(\"Ron Johnson: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(mandela_barnes_df)\n",
        "print(\"Mandela Barnes: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(donald_bolduc_df)\n",
        "print(\"Donald Bolduc: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(maggie_hassan_df)\n",
        "print(\"Maggie Hassan: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(ted_budd_df)\n",
        "print(\"Ted Budd: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(cheri_beasly_df)\n",
        "print(\"Cheri Beasley: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(joe_pinion_df)\n",
        "print(\"Joe Pinion: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(charles_schumer_df)\n",
        "print(\"Charles Schumer: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(jd_vance_df)\n",
        "print(\"JD Vance: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(tim_ryan_df)\n",
        "print(\"Tim Ryan: \", pos_all, neg_all, pos_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgqdn9WD6q9q",
        "outputId": "7806310b-4383-4926-971a-bec6286ad2d9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mehmet Oz:  0.66 0.34 1.93\n",
            "John Fetterman:  0.15 0.85 0.18\n",
            "Adam Laxalt:  0.66 0.34 1.93\n",
            "Catherine Cortez Masto:  0.77 0.23 3.33\n",
            "Ron Johnson:  0.97 0.03 32.33\n",
            "Mandela Barnes:  0.77 0.23 3.4\n",
            "Donald Bolduc:  0.6 0.4 1.5\n",
            "Maggie Hassan:  0.88 0.12 7.67\n",
            "Ted Budd:  0.12 0.88 0.14\n",
            "Cheri Beasley:  0.94 0.06 15.5\n",
            "Joe Pinion:  0.6 0.4 1.5\n",
            "Charles Schumer:  0.66 0.34 1.92\n",
            "JD Vance:  0.61 0.39 1.56\n",
            "Tim Ryan:  0.6 0.4 1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# winners vs losers\n",
        "winners = [john_fetterman_df, catherine_cortez_masto_df, ron_johnson_df, maggie_hassan_df, ted_budd_df, charles_schumer_df, jd_vance_df]\n",
        "losers = [mehmet_oz_df, adam_laxalt_df, mandela_barnes_df, donald_bolduc_df, cheri_beasly_df, joe_pinion_df, tim_ryan_df]"
      ],
      "metadata": {
        "id": "PBCo_SQA61Ml"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sum number of pos and neg tweets from list of df\n",
        "pos, neg = sum_sentiment(winners)\n",
        "print(\"Winners: \", pos, neg)\n",
        "pos, neg = sum_sentiment(losers)\n",
        "print(\"Losers: \", pos, neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr9gHJKa66ou",
        "outputId": "988ca1cb-f067-403d-c477-f553f5b450bc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Winners:  1255 1180\n",
            "Losers:  1130 490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# average the ratios of winners and losers\n",
        "pos_all, neg_all, pos_neg = avg_ratios(winners)\n",
        "print(\"Winners: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = avg_ratios(losers)\n",
        "print(\"Losers: \", pos_all, neg_all, pos_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3-PnRQD7ENA",
        "outputId": "50bc5c4e-23d0-4a0e-dbc9-728fe8d26dae"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Winners:  0.5942857142857143 0.4057142857142857 6.732857142857143\n",
            "Losers:  0.6899999999999998 0.31 3.894285714285714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# democrats vs republicans\n",
        "dems = [mehmet_oz_df, adam_laxalt_df, ron_johnson_df, donald_bolduc_df, ted_budd_df, joe_pinion_df, jd_vance_df]\n",
        "reps = [john_fetterman_df, catherine_cortez_masto_df, mandela_barnes_df, maggie_hassan_df, cheri_beasly_df, charles_schumer_df, tim_ryan_df]\n"
      ],
      "metadata": {
        "id": "zwOI4WI47It0"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sum numer of dem and rep tweets from list of df\n",
        "pos, neg = sum_sentiment(dems)\n",
        "print(\"Democrats: \", pos, neg)\n",
        "pos, neg = sum_sentiment(reps)\n",
        "print(\"Republicans: \", pos, neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOaIWlaD7NwS",
        "outputId": "777883e9-e33e-434c-9a4a-21579cb7dd6f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Democrats:  1185 830\n",
            "Republicans:  1200 840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#average the ratios of dems and reps\n",
        "pos_all, neg_all, pos_neg = avg_ratios(dems)\n",
        "print(\"Democrats: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = avg_ratios(reps)\n",
        "print(\"Republicans: \", pos_all, neg_all, pos_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75oT4YO67UEc",
        "outputId": "963f7ec9-3d89-40cf-baa0-11d657bb2383"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Democrats:  0.602857142857143 0.3971428571428572 5.841428571428572\n",
            "Republicans:  0.6814285714285714 0.3185714285714286 4.785714285714286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert all dfs to csv\n",
        "path = main_dir+'politician_csvs/'\n",
        "john_fetterman_df.to_csv(path+\"john_fetterman_annotated.csv\")\n",
        "mehmet_oz_df.to_csv(path+\"mehmet_oz_annotated.csv\")\n",
        "adam_laxalt_df.to_csv(path+\"adam_laxalt_annotated.csv\")\n",
        "catherine_cortez_masto_df.to_csv(path+\"catherine_cortez_masto_annotated.csv\")\n",
        "ron_johnson_df.to_csv(path+\"ron_johnson_annotated.csv\")\n",
        "mandela_barnes_df.to_csv(path+\"mandela_barnes_annotated.csv\")\n",
        "donald_bolduc_df.to_csv(path+\"donald_bolduc_annotated.csv\")\n",
        "maggie_hassan_df.to_csv(path+\"maggie_hassan_annotated.csv\")\n",
        "ted_budd_df.to_csv(path+\"ted_budd_annotated.csv\")\n",
        "cheri_beasly_df.to_csv(path+\"cheri_beasly_annotated.csv\")\n",
        "joe_pinion_df.to_csv(path+\"joe_pinion_annotated.csv\")\n",
        "charles_schumer_df.to_csv(path+\"charles_schumer_annotated.csv\")\n",
        "jd_vance_df.to_csv(path+\"jd_vance_annotated.csv\")\n",
        "tim_ryan_df.to_csv(path+\"tim_ryan_annotated.csv\")"
      ],
      "metadata": {
        "id": "A2RC0lgo7alI"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}