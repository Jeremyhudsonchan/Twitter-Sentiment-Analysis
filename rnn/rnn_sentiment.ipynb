{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK4hGtwYDM9x",
        "outputId": "af84d7ac-777d-4d46-f8f2-3688cd5270a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.8/dist-packages (3.8.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (5.2.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.28.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.50.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (22.11.23)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.14.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim==3.8.3\n",
        "!pip install keras --upgrade\n",
        "!pip install pandas --upgrade\n",
        "!pip install tensorflow --upgrade\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Matplot\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Keras\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "from keras import utils\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Word2vec\n",
        "import gensim\n",
        "\n",
        "#transformers\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# Utility\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter\n",
        "import logging\n",
        "import time\n",
        "import pickle\n",
        "import itertools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NeMm2b1DOoQ",
        "outputId": "98871f6a-4d07-4a2a-b8f9-4739cabfb54a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vocab_size = 290419\n",
        "# DATASET\n",
        "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "# TEXT CLENAING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "# WORD2VEC \n",
        "W2V_SIZE = 300\n",
        "W2V_WINDOW = 7\n",
        "W2V_EPOCH = 32\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)"
      ],
      "metadata": {
        "id": "c0zpGRIODS0I"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoECcrwyDUzt",
        "outputId": "e64a5d7d-d298-4e1e-ef9b-46808b587daa"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD DATASET\n",
        "main_dir = '/content/drive/MyDrive/nns/'\n",
        "df = pd.read_csv(main_dir+'training.1600000.processed.noemoticon.csv', encoding = DATASET_ENCODING,  names=DATASET_COLUMNS)\n",
        "neg_df = df[df.target == 0].sample(n=25000, random_state=5)\n",
        "pos_df = df[df.target == 4].sample(n=25000, random_state=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "lPYzG7eRJZur",
        "outputId": "debf12fc-0a0f-4377-abde-fb7b36bbabe5"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-54a9c8728c81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmain_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/nns/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'training.1600000.processed.noemoticon.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATASET_ENCODING\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATASET_COLUMNS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mneg_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpos_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   5771\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5773\u001b[0;31m         \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5774\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/sample.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid weights: weights sum to zero\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     return random_state.choice(obj_len, size=size, replace=replace, p=weights).astype(\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     )\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode_map = {0: \"NEGATIVE\", 2:\"NEUTRAL\", 4: \"POSITIVE\"}\n",
        "def decode_sentiment(label):\n",
        "    return decode_map[int(label)]"
      ],
      "metadata": {
        "id": "tht_a-bQJrqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "def preprocess(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = str(text).lower().strip()\n",
        "    text = re.sub(\"\\n\", \"\", text)\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "fqDFUeI-qbKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df.text = df.text.apply(lambda x: preprocess(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4d7cY9IKFiN",
        "outputId": "4b726b5e-f627-471c-bddb-8876d017a7e5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 250 ms, sys: 1.02 ms, total: 251 ms\n",
            "Wall time: 252 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\n",
        "print(\"TRAIN size:\", len(df_train))\n",
        "print(\"TEST size:\", len(df_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEmyHUISKJap",
        "outputId": "50d3b07e-845d-4f1d-93fe-623cca28c8de"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN size: 40000\n",
            "TEST size: 10001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD POLITICIANS TWEETS\n",
        "twt_load_dir = main_dir+'twitter_api_data/original/'\n",
        "\n",
        "# tweets regarding the politicans\n",
        "mehmet_oz_df = pd.read_csv(twt_load_dir+\"adam_laxalt.csv\", encoding =DATASET_ENCODING)\n",
        "john_fetterman_df = pd.read_csv(twt_load_dir+\"john_fetterman.csv\")\n",
        "adam_laxalt_df = pd.read_csv(twt_load_dir+\"adam_laxalt.csv\")\n",
        "catherine_cortez_masto_df = pd.read_csv(twt_load_dir+\"catherine_cortez_masto.csv\")\n",
        "ron_johnson_df = pd.read_csv(twt_load_dir+\"ron_johnson.csv\")\n",
        "mandela_barnes_df = pd.read_csv(twt_load_dir+\"mandela_barnes.csv\")\n",
        "donald_bolduc_df = pd.read_csv(twt_load_dir+\"donald_bolduc.csv\")\n",
        "maggie_hassan_df = pd.read_csv(twt_load_dir+\"maggie_hassan.csv\")\n",
        "ted_budd_df = pd.read_csv(twt_load_dir+\"ted_budd.csv\")\n",
        "cheri_beasly_df = pd.read_csv(twt_load_dir+\"cheri_beasly.csv\")\n",
        "joe_pinion_df = pd.read_csv(twt_load_dir+\"joe_pinion.csv\")\n",
        "charles_schumer_df = pd.read_csv(twt_load_dir+\"charles_schumer.csv\")\n",
        "jd_vance_df = pd.read_csv(twt_load_dir+\"jd_vance.csv\")\n",
        "tim_ryan_df = pd.read_csv(twt_load_dir+\"tim_ryan.csv\")"
      ],
      "metadata": {
        "id": "RZEpR2YuEXtv"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD RNN MODEL\n",
        "main_dir = '/content/drive/MyDrive/nns/'\n",
        "nn_load_dir = main_dir+'saved_nn_models/'\n",
        "model = tf.keras.models.load_model(nn_load_dir+'RNN.h5')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq09IbHXGj2h",
        "outputId": "dd862fcf-c6c6-408f-8da4-a78a696086f6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 300, 300)          9889500   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 300, 300)          0         \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 512)               1250304   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,140,317\n",
            "Trainable params: 1,250,817\n",
            "Non-trainable params: 9,889,500\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sentiment(score, include_neutral=False):\n",
        "    if include_neutral:        \n",
        "        label = NEUTRAL\n",
        "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "\n",
        "        return label\n",
        "    else:\n",
        "        return 0 if score < 0.5 else 1"
      ],
      "metadata": {
        "id": "PseWT6o3KZW7"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LOAD TOKENIZER\n",
        "tokenizer_path = main_dir+'tokenizer.pkl'\n",
        "\n",
        "with open(tokenizer_path, 'rb') as f:\n",
        "  tokenizer = pickle.load(f)"
      ],
      "metadata": {
        "id": "r3bGr9lgXuVd"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, include_neutral=False):\n",
        "    # Tokenize text\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
        "    # Predict\n",
        "    score = model.predict([x_test], verbose=0)[0]\n",
        "    # Decode sentiment\n",
        "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "    return label"
      ],
      "metadata": {
        "id": "nj4ZHh0E0rrQ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(df):\n",
        "  df.Tweet = df.Tweet.apply(lambda x: preprocess(x))\n",
        "  df['sentiment'] = 0\n",
        "  length = len(df)\n",
        "  for i in range(length):\n",
        "    text = df.Tweet[i]\n",
        "    label = predict(text, include_neutral=False)\n",
        "    df.at[i, 'sentiment'] = label\n",
        "    "
      ],
      "metadata": {
        "id": "lV3quS_72fON"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run model on all csvs\n",
        "run_model(mehmet_oz_df)\n",
        "run_model(john_fetterman_df)\n",
        "run_model(adam_laxalt_df)\n",
        "run_model(catherine_cortez_masto_df)\n",
        "run_model(ron_johnson_df)\n",
        "run_model(mandela_barnes_df)\n",
        "run_model(donald_bolduc_df)\n",
        "run_model(maggie_hassan_df)\n",
        "run_model(ted_budd_df)\n",
        "run_model(cheri_beasly_df)\n",
        "run_model(joe_pinion_df)\n",
        "run_model(charles_schumer_df)\n",
        "run_model(jd_vance_df)\n",
        "run_model(tim_ryan_df)"
      ],
      "metadata": {
        "id": "N8YPBSYo3rSZ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get number of positive and negatives tweets from each df\n",
        "def get_sentiment(df):\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "    for index, row in df.iterrows():\n",
        "        if row['sentiment'] == 0:\n",
        "            neg += 1\n",
        "        else:\n",
        "            pos += 1\n",
        "    return pos, neg\n",
        "\n",
        "# get pos/neg ,pos/all, neg/all ratio for all dfs\n",
        "def get_ratios(df):\n",
        "    pos, neg = get_sentiment(df)\n",
        "    pos_all = pos / (pos + neg)\n",
        "    neg_all = neg / (pos + neg)\n",
        "    pos_neg = pos / neg\n",
        "    # round all ratios to 2 decimal places\n",
        "    pos_all = round(pos_all, 2)\n",
        "    neg_all = round(neg_all, 2)\n",
        "    pos_neg = round(pos_neg, 2)\n",
        "    return pos_all, neg_all, pos_neg\n",
        "\n",
        "# sum number of positive and negatives tweets from list of df\n",
        "def sum_sentiment(dfs):\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "    for df in dfs:\n",
        "        pos_df, neg_df = get_sentiment(df)\n",
        "        pos += pos_df\n",
        "        neg += neg_df\n",
        "    return pos, neg\n",
        "\n",
        "# average the ratios of winners and losers\n",
        "def avg_ratios(dfs):\n",
        "    pos_all = 0\n",
        "    neg_all = 0\n",
        "    pos_neg = 0\n",
        "    for df in dfs:\n",
        "        pos_all_df, neg_all_df, pos_neg_df = get_ratios(df)\n",
        "        pos_all += pos_all_df\n",
        "        neg_all += neg_all_df\n",
        "        pos_neg += pos_neg_df\n",
        "    pos_all = pos_all / len(dfs)\n",
        "    neg_all = neg_all / len(dfs)\n",
        "    pos_neg = pos_neg / len(dfs)\n",
        "    return pos_all, neg_all, pos_neg"
      ],
      "metadata": {
        "id": "MYsQK8Y06fIE"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get number of positive and negatives tweets from each df\n",
        "pos, neg = get_sentiment(mehmet_oz_df)\n",
        "print(\"Mehmet Oz: \", pos, neg)\n",
        "pos, neg = get_sentiment(john_fetterman_df)\n",
        "print(\"John Fetterman: \", pos, neg)\n",
        "pos, neg = get_sentiment(adam_laxalt_df)\n",
        "print(\"Adam Laxalt: \", pos, neg)\n",
        "pos, neg = get_sentiment(catherine_cortez_masto_df)\n",
        "print(\"Catherine Cortez Masto: \", pos, neg)\n",
        "pos, neg = get_sentiment(ron_johnson_df)\n",
        "print(\"Ron Johnson: \", pos, neg)\n",
        "pos, neg = get_sentiment(mandela_barnes_df)\n",
        "print(\"Mandela Barnes: \", pos, neg)\n",
        "pos, neg = get_sentiment(donald_bolduc_df)\n",
        "print(\"Donald Bolduc: \", pos, neg)\n",
        "pos, neg = get_sentiment(maggie_hassan_df)\n",
        "print(\"Maggie Hassan: \", pos, neg)\n",
        "pos, neg = get_sentiment(ted_budd_df)\n",
        "print(\"Ted Budd: \", pos, neg)\n",
        "pos, neg = get_sentiment(cheri_beasly_df)\n",
        "print(\"Cheri Beasley: \", pos, neg)\n",
        "pos, neg = get_sentiment(joe_pinion_df)\n",
        "print(\"Joe Pinion: \", pos, neg)\n",
        "pos, neg = get_sentiment(charles_schumer_df)\n",
        "print(\"Charles Schumer: \", pos, neg)\n",
        "pos, neg = get_sentiment(jd_vance_df)\n",
        "print(\"JD Vance: \", pos, neg)\n",
        "pos, neg = get_sentiment(tim_ryan_df)\n",
        "print(\"Tim Ryan: \", pos, neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zIQoy5L6mAb",
        "outputId": "4cd19863-3239-4505-e84c-778fc663803e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mehmet Oz:  135 85\n",
            "John Fetterman:  330 170\n",
            "Adam Laxalt:  135 85\n",
            "Catherine Cortez Masto:  55 75\n",
            "Ron Johnson:  475 25\n",
            "Mandela Barnes:  190 250\n",
            "Donald Bolduc:  5 20\n",
            "Maggie Hassan:  20 110\n",
            "Ted Budd:  90 410\n",
            "Cheri Beasley:  80 85\n",
            "Joe Pinion:  20 30\n",
            "Charles Schumer:  100 75\n",
            "JD Vance:  225 275\n",
            "Tim Ryan:  250 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get ratios of pos:neg tweets from all dfs\n",
        "pos_all, neg_all, pos_neg = get_ratios(mehmet_oz_df)\n",
        "print(\"Mehmet Oz: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(john_fetterman_df)\n",
        "print(\"John Fetterman: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(adam_laxalt_df)\n",
        "print(\"Adam Laxalt: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(catherine_cortez_masto_df)\n",
        "print(\"Catherine Cortez Masto: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(ron_johnson_df)\n",
        "print(\"Ron Johnson: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(mandela_barnes_df)\n",
        "print(\"Mandela Barnes: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(donald_bolduc_df)\n",
        "print(\"Donald Bolduc: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(maggie_hassan_df)\n",
        "print(\"Maggie Hassan: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(ted_budd_df)\n",
        "print(\"Ted Budd: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(cheri_beasly_df)\n",
        "print(\"Cheri Beasley: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(joe_pinion_df)\n",
        "print(\"Joe Pinion: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(charles_schumer_df)\n",
        "print(\"Charles Schumer: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(jd_vance_df)\n",
        "print(\"JD Vance: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = get_ratios(tim_ryan_df)\n",
        "print(\"Tim Ryan: \", pos_all, neg_all, pos_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgqdn9WD6q9q",
        "outputId": "5296d9fe-b11e-4548-8109-9edacdd5645f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mehmet Oz:  0.61 0.39 1.59\n",
            "John Fetterman:  0.66 0.34 1.94\n",
            "Adam Laxalt:  0.61 0.39 1.59\n",
            "Catherine Cortez Masto:  0.42 0.58 0.73\n",
            "Ron Johnson:  0.95 0.05 19.0\n",
            "Mandela Barnes:  0.43 0.57 0.76\n",
            "Donald Bolduc:  0.2 0.8 0.25\n",
            "Maggie Hassan:  0.15 0.85 0.18\n",
            "Ted Budd:  0.18 0.82 0.22\n",
            "Cheri Beasley:  0.48 0.52 0.94\n",
            "Joe Pinion:  0.4 0.6 0.67\n",
            "Charles Schumer:  0.57 0.43 1.33\n",
            "JD Vance:  0.45 0.55 0.82\n",
            "Tim Ryan:  0.5 0.5 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# winners vs losers\n",
        "winners = [john_fetterman_df, catherine_cortez_masto_df, ron_johnson_df, maggie_hassan_df, ted_budd_df, charles_schumer_df, jd_vance_df]\n",
        "losers = [mehmet_oz_df, adam_laxalt_df, mandela_barnes_df, donald_bolduc_df, cheri_beasly_df, joe_pinion_df, tim_ryan_df]"
      ],
      "metadata": {
        "id": "PBCo_SQA61Ml"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sum number of pos and neg tweets from list of df\n",
        "pos, neg = sum_sentiment(winners)\n",
        "print(\"Winners: \", pos, neg)\n",
        "pos, neg = sum_sentiment(losers)\n",
        "print(\"Losers: \", pos, neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr9gHJKa66ou",
        "outputId": "173dfb29-2113-4d4c-c74c-b98563ecc8c7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Winners:  1295 1140\n",
            "Losers:  815 805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# average the ratios of winners and losers\n",
        "pos_all, neg_all, pos_neg = avg_ratios(winners)\n",
        "print(\"Winners: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = avg_ratios(losers)\n",
        "print(\"Losers: \", pos_all, neg_all, pos_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3-PnRQD7ENA",
        "outputId": "2de02c5f-908a-4c65-d762-e3b1e089350a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Winners:  0.48285714285714293 0.5171428571428571 3.46\n",
            "Losers:  0.4614285714285714 0.5385714285714286 0.9714285714285715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# democrats vs republicans\n",
        "dems = [mehmet_oz_df, adam_laxalt_df, ron_johnson_df, donald_bolduc_df, ted_budd_df, joe_pinion_df, jd_vance_df]\n",
        "reps = [john_fetterman_df, catherine_cortez_masto_df, mandela_barnes_df, maggie_hassan_df, cheri_beasly_df, charles_schumer_df, tim_ryan_df]\n"
      ],
      "metadata": {
        "id": "zwOI4WI47It0"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sum numer of dem and rep tweets from list of df\n",
        "pos, neg = sum_sentiment(dems)\n",
        "print(\"Democrats: \", pos, neg)\n",
        "pos, neg = sum_sentiment(reps)\n",
        "print(\"Republicans: \", pos, neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOaIWlaD7NwS",
        "outputId": "84d0a899-bcb2-4f79-cf90-5b07fb9abe91"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Democrats:  1085 930\n",
            "Republicans:  1025 1015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#average the ratios of dems and reps\n",
        "pos_all, neg_all, pos_neg = avg_ratios(dems)\n",
        "print(\"Democrats: \", pos_all, neg_all, pos_neg)\n",
        "pos_all, neg_all, pos_neg = avg_ratios(reps)\n",
        "print(\"Republicans: \", pos_all, neg_all, pos_neg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75oT4YO67UEc",
        "outputId": "732f6c03-34e8-4f2c-df05-7c78a04505e3"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Democrats:  0.48571428571428577 0.5142857142857143 3.448571428571429\n",
            "Republicans:  0.4585714285714285 0.5414285714285715 0.9828571428571429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert all dfs to csv\n",
        "path = main_dir+'politician_csvs/'\n",
        "john_fetterman_df.to_csv(path+\"john_fetterman_annotated.csv\")\n",
        "mehmet_oz_df.to_csv(path+\"mehmet_oz_annotated.csv\")\n",
        "adam_laxalt_df.to_csv(path+\"adam_laxalt_annotated.csv\")\n",
        "catherine_cortez_masto_df.to_csv(path+\"catherine_cortez_masto_annotated.csv\")\n",
        "ron_johnson_df.to_csv(path+\"ron_johnson_annotated.csv\")\n",
        "mandela_barnes_df.to_csv(path+\"mandela_barnes_annotated.csv\")\n",
        "donald_bolduc_df.to_csv(path+\"donald_bolduc_annotated.csv\")\n",
        "maggie_hassan_df.to_csv(path+\"maggie_hassan_annotated.csv\")\n",
        "ted_budd_df.to_csv(path+\"ted_budd_annotated.csv\")\n",
        "cheri_beasly_df.to_csv(path+\"cheri_beasly_annotated.csv\")\n",
        "joe_pinion_df.to_csv(path+\"joe_pinion_annotated.csv\")\n",
        "charles_schumer_df.to_csv(path+\"charles_schumer_annotated.csv\")\n",
        "jd_vance_df.to_csv(path+\"jd_vance_annotated.csv\")\n",
        "tim_ryan_df.to_csv(path+\"tim_ryan_annotated.csv\")"
      ],
      "metadata": {
        "id": "A2RC0lgo7alI"
      },
      "execution_count": 79,
      "outputs": []
    }
  ]
}